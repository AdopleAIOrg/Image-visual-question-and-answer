{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO7ZCZ+J50lma7u1JkSyMh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdopleAIOrg/Image-visual-question-and-answer/blob/main/Image_visual_question_and_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBkj-eQfPgAe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "\n",
        "class ImageVisulaQuestionAnswer:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "      \"\"\"\n",
        "      Initializes the ImageVisualQuestionAnswering class by loading the pre-trained ViLT model and processor.\n",
        "      \"\"\"\n",
        "\n",
        "      # Load the pre-trained ViLT processor for image and text encoding\n",
        "      self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "\n",
        "      # Load the pre-trained ViLT model for Visual Question Answering\n",
        "      self.model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "\n",
        "    def _get_question(self) -> tuple:\n",
        "\n",
        "      \"\"\"\n",
        "      Displays the image uploader and question input fields and returns the uploaded image and the user's question.\n",
        "\n",
        "      Returns:\n",
        "          tuple: A tuple containing the PIL image and the user's question.\n",
        "      \"\"\"\n",
        "\n",
        "      st.title(\"Visual Question Answering with ViLT\")\n",
        "\n",
        "      # upload image\n",
        "      uploaded_file = st.file_uploader(\"Choose an image file\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "      if uploaded_file is not None:\n",
        "          image = Image.open(uploaded_file)\n",
        "      else:\n",
        "          # default image\n",
        "          url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "          image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "      # ask question\n",
        "      question = st.text_input(\"Enter your question\")\n",
        "\n",
        "      return image, question\n",
        "\n",
        "    def get_answer(self) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Gets the user's question and displays the answer using the ViLT model.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the uploaded image and user's question from the _get_question method\n",
        "        image, question = self._get_question()\n",
        "\n",
        "        # Encode the image and question using the ViLT processor\n",
        "        encoding = self.processor(image, question, return_tensors=\"pt\")\n",
        "\n",
        "        if question:\n",
        "            # Pass the encoded image and question through the ViLT model\n",
        "            outputs = self.model(**encoding)\n",
        "\n",
        "            # Extract the logits from the model outputs\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Get the index of the predicted answer using argmax\n",
        "            idx = logits.argmax(-1).item()\n",
        "\n",
        "            # Get the index of the predicted answer using argmax\n",
        "            answer = self.model.config.id2label[idx]\n",
        "\n",
        "            # Display the predicted answer\n",
        "            st.write(\"Answer:\", answer)\n",
        "        else:\n",
        "            # If no question is provided, display a warning\n",
        "            st.warning(\"Please enter a question\")\n",
        "\n",
        "        # Show the image in the Streamlit app\n",
        "        st.image(image, caption=\"Image\", use_column_width=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    image_qa = ImageVisulaQuestionAnswer()\n",
        "    image_qa.get_answer()"
      ],
      "metadata": {
        "id": "_YvHlU8YP1Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "NDoh4mebR2e8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}